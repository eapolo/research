@ARTICLE{10.3389/fpls.2021.684328,

AUTHOR={Maheswari, Prabhakar  and Raja, Purushothaman  and Apolo-Apolo, Orly Enrique  and Pérez-Ruiz, Manuel },

TITLE={Intelligent Fruit Yield Estimation for Orchards Using Deep Learning Based Semantic Segmentation Techniques—A Review},

JOURNAL={Frontiers in Plant Science},

VOLUME={12},

YEAR={2021},

URL={https://www.frontiersin.org/journals/plant-science/articles/10.3389/fpls.2021.684328},

DOI={10.3389/fpls.2021.684328},

ISSN={1664-462X},

ABSTRACT={<p>Smart farming employs intelligent systems for every domain of agriculture to obtain sustainable economic growth with the available resources using advanced technologies. Deep Learning (DL) is a sophisticated artificial neural network architecture that provides state-of-the-art results in smart farming applications. One of the main tasks in this domain is yield estimation. Manual yield estimation undergoes many hurdles such as labor-intensive, time-consuming, imprecise results, etc. These issues motivate the development of an intelligent fruit yield estimation system that offers more benefits to the farmers in deciding harvesting, marketing, etc. Semantic segmentation combined with DL adds promising results in fruit detection and localization by performing pixel-based prediction. This paper reviews the different literature employing various techniques for fruit yield estimation using DL-based semantic segmentation architectures. It also discusses the challenging issues that occur during intelligent fruit yield estimation such as sampling, collection, annotation and data augmentation, fruit detection, and counting. Results show that the fruit yield estimation employing DL-based semantic segmentation techniques yields better performance than earlier techniques because of human cognition incorporated into the architecture. Future directions like customization of DL architecture for smart-phone applications to predict the yield, development of more comprehensive model encompassing challenging situations like occlusion, overlapping and illumination variation, etc., were also discussed.</p>}}